{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook can be found at [probprog-ai-tuwien/turing-time-series](https://github.com/probprog-ai-tuwien/turing-time-series)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "using Turing\n",
    "using StatsPlots\n",
    "using Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PRINT_PROGRESS = false\n",
    "include(\"read_data.jl\")\n",
    "include(\"utils.jl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Bayesian Modeling with Probabilistic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ## Time Series Model\n",
    "- ## Trend\n",
    "- ## Seasonality\n",
    "- ## Debugging\n",
    "- ## Combine + Forecast\n",
    "- ## Bonus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset: World-Wide Monthly Air Passengers\n",
    "\n",
    "Data source: https://www.transtats.bts.gov/Data_Elements.aspx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_plot(air_passengers_2013_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First Start: A Linear Trend Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start easy by modelling the trend of the time series.\n",
    "\n",
    "A Bayesian linear regression is the \"hello world\" example for probabilistic programs and can be used to model linear trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@model function trend_model_1(x::Vector{Float64}, y::Vector{Float64})\n",
    "    # priors are guided by data but chosen to be very broad\n",
    "    # (we do not want to favour any specific values)\n",
    "    slope ~ Normal(0,3)\n",
    "    intercept ~ Normal(6,3)\n",
    "    # error models how noisy the data is, it has to be >0 => InverseGamma\n",
    "    error ~ InverseGamma(2,3)\n",
    "    \n",
    "    for i in eachindex(y)\n",
    "        # shift the x-axis by start year 0...16 instead of 2003...2019\n",
    "        y[i] ~ Normal(slope * (x[i] - x[1]) + intercept, error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We focus on the part of the time series which approximately exhibits a linear trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "upwards_trend_data = air_passengers_2013_2018[2009 .<= air_passengers_2013_2018[!,\"Year\"],:]\n",
    "base_plot(upwards_trend_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the model allows it (continuous density), the inference algorithm `NUTS` is always a good first choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Turing.Random.seed!(0)\n",
    "res_1 = sample(trend_model_1(upwards_trend_data[!,\"Date\"], upwards_trend_data[!,\"Total\"]), NUTS(), 3000, progress=PRINT_PROGRESS)\n",
    "res_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While we can inspect the summary statistics above, plotting the results gives a quick and easy check if everything works as expected.\n",
    "\n",
    "To start, we plot the maximum-a-posteriori (MAP) estimate - the most likely values for the latent variables under the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function plot_trend_model_1_map(data, slope, intercept, error)\n",
    "    p = base_plot(data)\n",
    "    t_min = data[!,\"Date\"][1]\n",
    "    plot!(p, t -> slope * (t-t_min) + intercept, color=\"red\")\n",
    "    plot!(p, t -> slope * (t-t_min) + intercept + sqrt(error), color=\"orange\")\n",
    "    plot!(p, t -> slope * (t-t_min) + intercept - sqrt(error), color=\"orange\")\n",
    "    p\n",
    "end\n",
    "_, map_vector = get_map(res_1)\n",
    "plot_trend_model_1_map(upwards_trend_data, map_vector...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But of course as Bayesian statisticians we also care about the uncertainty around our estimates.\n",
    "\n",
    "So we plot samples from the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function plot_trend_model_1_posterior(data, res)\n",
    "    p = base_plot(data)\n",
    "    t_min = data[!,\"Date\"][1]\n",
    "    for i in 1:length(res)\n",
    "        _, trace = get_trace(res, i)\n",
    "        slope, intercept, error = trace\n",
    "        plot!(p, t -> slope * (t-t_min) + intercept, color=\"black\", alpha=0.05)\n",
    "    end\n",
    "    p\n",
    "end\n",
    "plot_trend_model_1_posterior(upwards_trend_data, res_1[1:30:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Changing Trend"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot above we can see that a linear model does not perfectly describe the trend in the time series.\n",
    "\n",
    "In particular, after 2014 there seems to be a steeper increase in passengers then before.\n",
    "\n",
    "Lets see if we can model this with a changepoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function trend_model_2_f(t::Real, slope::Real, intercept::Real, changepoint::Real, adjustment::Real)\n",
    "    k = slope\n",
    "    m = intercept\n",
    "    # UPGRADE: if the time series exeeds the changepoint, we adjust slope and intercept in a continuous way\n",
    "    if changepoint ≤ t\n",
    "        k += adjustment\n",
    "        m -= changepoint * adjustment\n",
    "    end\n",
    "    return k * t + m\n",
    "end\n",
    "\n",
    "@model function trend_model_2(x::Vector{Float64}, y::Vector{Float64})\n",
    "    slope ~ Normal(0,3)\n",
    "    intercept ~ Normal(6,3)\n",
    "    error ~ InverseGamma(2,3)\n",
    "\n",
    "    # UPGRADE: sample changepoint and adjustment to be made at changepoint\n",
    "    changepoint ~ Uniform(0, x[end] - x[1])\n",
    "    adjustment ~ Normal(0,1)\n",
    "\n",
    "    for i in eachindex(y)\n",
    "        y[i] ~ Normal(trend_model_2_f(x[i]-x[1], slope, intercept, changepoint, adjustment), error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Turing.Random.seed!(0)\n",
    "res_2 = sample(trend_model_2(upwards_trend_data[!,\"Date\"], upwards_trend_data[!,\"Total\"]), NUTS(), 3000, progress=PRINT_PROGRESS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, plotting is our best friend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function plot_trend_model_2(data, slope, intercept, error, changepoint, adjustment)\n",
    "    p = base_plot(data)\n",
    "    t_min = data[!,\"Date\"][1]\n",
    "    plot!(p, t -> trend_model_2_f(t - t_min, slope, intercept, changepoint, adjustment), color=\"red\")\n",
    "    plot!(p, t -> trend_model_2_f(t - t_min, slope, intercept, changepoint, adjustment) + sqrt(error), color=\"orange\")\n",
    "    plot!(p, t -> trend_model_2_f(t - t_min, slope, intercept, changepoint, adjustment) - sqrt(error), color=\"orange\")\n",
    "    vline!([changepoint + t_min], linestyle=:dash, color=\"black\")\n",
    "    p\n",
    "end\n",
    "_, map_vector = get_map(res_2)\n",
    "single_chainpoint_res = res_2\n",
    "plot_trend_model_2(upwards_trend_data, map_vector...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiple Changepoints"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we turn our attention back to the full time series, we can see that there seem to be multiple changes in the trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_plot(air_passengers_2013_2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can simply modify our model to handle multiple changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function trend_model_3_1_f(t::Real, slope::Real, intercept::Real, changepoints::Vector{<:Real}, adjustments::Vector{<:Real})\n",
    "    k = slope\n",
    "    m = intercept\n",
    "    # UPGRADE: now iterate over multiple changepoints\n",
    "    for (changepoint, adjustment) in zip(changepoints, adjustments)\n",
    "        if changepoint ≤ t\n",
    "            k += adjustment\n",
    "            m -= changepoint * adjustment\n",
    "        end\n",
    "    end\n",
    "    return k * t + m\n",
    "end\n",
    "\n",
    "@model function trend_model_3_1(x::Vector{Float64}, y)\n",
    "    slope ~ Normal(0,1)\n",
    "    intercept ~ Normal(6,1)\n",
    "    error ~ InverseGamma(2,3)\n",
    "    \n",
    "    # UPGRADE: sample the number of changepoints and multiple changepoint candidates\n",
    "    n_changepoints ~ DiscreteUniform(1,5)\n",
    "    changepoints ~ filldist(Uniform(0, x[end] - x[1]), 5)\n",
    "    adjustments ~ filldist(Normal(0,1), 5)\n",
    "    \n",
    "    k = slope\n",
    "    m = intercept\n",
    "    j = 1\n",
    "    \n",
    "    if ismissing(y)\n",
    "        y = zeros(length(x))\n",
    "    end\n",
    "    for i in eachindex(y)\n",
    "        y[i] ~ Normal(trend_model_3_1_f(x[i]-x[1], slope, intercept, changepoints[1:n_changepoints], adjustments[1:n_changepoints]), error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try\n",
    "    Turing.Random.seed!(0)\n",
    "    res_3_1 = sample(trend_model_3_1(air_passengers_2013_2018[!,\"Date\"], air_passengers_2013_2018[!,\"Total\"]), NUTS(), 3000, progress=PRINT_PROGRESS)\n",
    "catch e\n",
    "    println(e)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NUTS` does not work anymore because we have the discrete variable `n_changepoints`.\n",
    "\n",
    "Let's give it a try with simple metroplis hastings `MH`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Turing.Random.seed!(0)\n",
    "sampler_3_1 = MH()\n",
    "res_3_1 = sample(trend_model_3_1(air_passengers_2013_2018[!,\"Date\"], air_passengers_2013_2018[!,\"Total\"]), sampler_3_1, 3000, progress=PRINT_PROGRESS);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately, plotting shows that the inference was not able to produce the correct changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function plot_trend_model_3_1(data, slope, intercept, error, changepoints, adjustments)\n",
    "    p = base_plot(data)\n",
    "    x = data isa Tuple ? data[1] : data[!,\"Date\"]\n",
    "    t_min = x[1]\n",
    "    plot!(p, t -> trend_model_3_1_f(t - t_min, slope, intercept, changepoints, adjustments), color=\"red\")\n",
    "    plot!(p, t -> trend_model_3_1_f(t - t_min, slope, intercept, changepoints, adjustments) + sqrt(error), color=\"orange\")\n",
    "    plot!(p, t -> trend_model_3_1_f(t - t_min, slope, intercept, changepoints, adjustments) - sqrt(error), color=\"orange\")\n",
    "\n",
    "    vline!(changepoints[abs.(adjustments) .> 0.01] .+ t_min, linestyle=:dash, color=\"black\")\n",
    "    return p\n",
    "end\n",
    "_, map_vector = get_map(res_3_1)\n",
    "n_changepoints = Int(map_vector[4])\n",
    "changepoints = map_vector[5:9][1:n_changepoints]\n",
    "adjustments = map_vector[10:end][1:n_changepoints]\n",
    "\n",
    "plot_trend_model_3_1(air_passengers_2013_2018, map_vector[1], map_vector[2], map_vector[3], changepoints, adjustments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debugging a Probabilistic Program"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first approach to debugging inference of a probabilistic program is to look at the MCMC diagnostic plots.\n",
    "\n",
    "They show how the values of the latent variables are explored during inference.\n",
    "\n",
    "We can see that the values hardly change when using `MH` => bad exploration of the state space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(res_3_1[[\"slope\", \"intercept\", \"n_changepoints\", \"changepoints[1]\", \"adjustments[1]\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For reference, below you can see how MCMC diagnostic plots are supposed to look if the inference algorithm explores the state space well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot(single_chainpoint_res[[\"slope\", \"intercept\", \"changepoint\", \"adjustment\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two reasons why the inference may fail like in our case:\n",
    "- the model does not describe the data well\n",
    "- the inference algorithm is not able to compute the correct posterior\n",
    "\n",
    "We can test for the first reason by inspecting the \"worlds\" our model can generate - this is called a **Prior Predictive Check**.\n",
    "\n",
    "To test for the second reason, we try our inference algorithm on simulated data, because then we know that the model perfectly describes the data - this is called **Fake Data Simulation**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prior Predictive Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Turing.Random.seed!(0)\n",
    "x = air_passengers_2013_2018[!,\"Date\"]\n",
    "\n",
    "# use probalistic program as a generative model\n",
    "res = sample(trend_model_3_1(x, missing), Prior(), 19, progress=false)\n",
    "\n",
    "t_min = x[1]\n",
    "t_max = x[end]\n",
    "\n",
    "p = plot(x, air_passengers_2013_2018[!,\"Total\"], legend=false, xticks=[], yticks=[])\n",
    "ps = [p]\n",
    "for i in 1:length(res)\n",
    "    _, trace = get_trace(res,i)\n",
    "    slope, intercept = trace[1:2]\n",
    "    n_changepoints = Int(trace[4])\n",
    "    changepoints = trace[5:9][1:n_changepoints]\n",
    "    adjustments = trace[10:14][1:n_changepoints]\n",
    "    y_from_prior = trace[15:end]\n",
    "    p = plot(x, t -> trend_model_3_1_f(t - t_min, slope, intercept, changepoints, adjustments), color=:gray, legend=false, xticks=[])#, yticks=[])\n",
    "    plot!(p, x, y_from_prior)\n",
    "    \n",
    "    cps = changepoints .+ t_min\n",
    "    vline!(p, cps[(t_min .< cps) .& (cps .< t_max)], linestyle=:dash)\n",
    "    push!(ps, p)\n",
    "end\n",
    "plot(ps..., layout=(5,4), size=(1000,1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generated \"worlds\" seem to agree with our intention for modelling changepoints. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fake-Data Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ps = []\n",
    "for i in 1:12\n",
    "    # for each of the simulated data sets ...\n",
    "    _, trace = get_trace(res,i)\n",
    "    slope, intercept = trace[1:2]\n",
    "    n_changepoints = Int(trace[4])\n",
    "    true_changepoints = trace[5:9][1:n_changepoints]\n",
    "    adjustments = trace[9:14][1:n_changepoints]\n",
    "    y_from_prior = trace[15:end]\n",
    "\n",
    "    # ... perform posterior inference ...\n",
    "    Turing.Random.seed!(10)\n",
    "    ppc_res = sample(trend_model_3_1(x, y_from_prior), sampler_3_1, 3000, progress=false)\n",
    "\n",
    "    # ... and check if we can recover the true latents\n",
    "    _, map_vector = get_map(ppc_res)\n",
    "    n_changepoints = Int(map_vector[4])\n",
    "    changepoints = map_vector[5:9][1:n_changepoints]\n",
    "    adjustments = map_vector[9:14][1:n_changepoints]\n",
    "\n",
    "    p = plot_trend_model_3_1((x, y_from_prior), map_vector[1], map_vector[2], map_vector[3], changepoints, adjustments)\n",
    "    vline!(true_changepoints .+ t_min, linestyle=:dash, color=:green)\n",
    "    plot!(p, xticks=[], yticks=[], xlabel=\"\", ylabel=\"\")\n",
    "    \n",
    "    push!(ps,p)\n",
    "end\n",
    "plot(ps..., layout=(3,4), size=(1000,600))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We conclude that the model should be able to describe our data, however, the inference algorithm is not powerful enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Multiple Changepoints - Second Try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of trying to engineer a powerful inference algorithm, we adjust the model slightly to work with `NUTS` again.\n",
    "\n",
    "The problem was that the number of true changepoints is discrete and unknown.\n",
    "\n",
    "We now fix the changepoints at the beginning of each year, but put a restrictive prior on the adjustments,  \n",
    "such that the adjustments tend to 0 if there is no real change in trend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@model function trend_model_3(x::Vector{Float64}, y::Vector{Float64})\n",
    "    slope ~ Normal(0,3)\n",
    "    intercept ~ Normal(6,3)\n",
    "    error ~ InverseGamma(2,3)\n",
    "    tau ~ InverseGamma(1,1)\n",
    "\n",
    "    # UPGRADE: fix one changepoint per year, put restrictive Laplace prior on adjustments\n",
    "    n_changepoints = length(y) ÷ 12\n",
    "    adjustments ~ filldist(Laplace(0,tau+1e-5), n_changepoints)\n",
    "    \n",
    "    k = slope\n",
    "    m = intercept\n",
    "    j = 1\n",
    "    for i in eachindex(y)\n",
    "        # UPGRADE: adjust slope and intercept at beginning of each year\n",
    "        if i % 12 == 1\n",
    "            k += adjustments[j]\n",
    "            m -= (x[i] - x[1]) * adjustments[j]\n",
    "            j += 1\n",
    "        end\n",
    "        y[i] ~ Normal(k * (x[i] - x[1]) + m, error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For even faster model iteration, we may solve directly for the MAP which is faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Turing.Random.seed!(0)\n",
    "# res_3 = sample(trend_model_3(air_passengers_2013_2018[!,\"Date\"], air_passengers_2013_2018[!,\"Total\"]), NUTS(), 3000, progress=PRINT_PROGRESS);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "res_3 = optimize(trend_model_3(air_passengers_2013_2018[!,\"Date\"], air_passengers_2013_2018[!,\"Total\"]), MAP(), Optim.Options(iterations=10_000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function trend_model_3_f(t, slope, intercept, changepoints, adjustments)\n",
    "    ix = changepoints .<= t\n",
    "    return (slope + sum(adjustments[ix])) * t + (intercept - changepoints[ix]'adjustments[ix])\n",
    "end\n",
    "\n",
    "function plot_trend_model_3(data, slope, intercept, error, adjustments)\n",
    "    p = base_plot(data)\n",
    "    x = data[!,\"Date\"]\n",
    "    t_min = x[1]\n",
    "    changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "    plot!(p, t -> trend_model_3_f(t - t_min, slope, intercept, changepoints, adjustments), color=\"red\")\n",
    "    plot!(p, t -> trend_model_3_f(t - t_min, slope, intercept, changepoints, adjustments) + sqrt(error), color=\"orange\")\n",
    "    plot!(p, t -> trend_model_3_f(t - t_min, slope, intercept, changepoints, adjustments) - sqrt(error), color=\"orange\")\n",
    "\n",
    "    vline!(changepoints[abs.(adjustments) .> 0.01] .+ t_min, linestyle=:dash, color=\"black\")\n",
    "    p\n",
    "end\n",
    "# _, map_vector = get_map(res_3)\n",
    "map_vector = res_3.values.array\n",
    "plot_trend_model_3(air_passengers_2013_2018, map_vector[1], map_vector[2], map_vector[3], map_vector[5:end])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are happy with the trend model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Seasonality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we have completely neglected the seasonality of the time series.\n",
    "\n",
    "But having modelled the trend of the time series accurately, we can substract the trend from the data and solely focus on the seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = air_passengers_2013_2018[!,\"Date\"]\n",
    "y = air_passengers_2013_2018[!,\"Total\"]\n",
    "t_min = x[1]\n",
    "changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "slope, intercept, error, adjustments = map_vector[1], map_vector[2], map_vector[3], map_vector[5:end]\n",
    "\n",
    "# substract the trend model from the data\n",
    "y_stationary =  y .- map(t -> trend_model_3_f(t - t_min, slope, intercept, changepoints, adjustments), x)\n",
    "\n",
    "base_plot((x, y_stationary))\n",
    "title!(\"air passengers without trend\")\n",
    "ylabel!(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seasonality is modelled by decomposing the time series into sinusoidal functions with different frequencies and phase shifts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function seasonality_component(t::Float64, N_frequencies::Int, beta::Vector{<:Real})\n",
    "    s = 0\n",
    "    for n in 1:N_frequencies\n",
    "        s += beta[2*n-1] * sin(2*pi*n*t)\n",
    "        s += beta[2*n] * cos(2*pi*n*t)\n",
    "    end\n",
    "    return s\n",
    "end\n",
    "\n",
    "@model function seasonality_model(x::Vector{Float64}, y::Vector{Float64}, N_frequencies::Int)\n",
    "    beta ~ filldist(Normal(0,1.), 2*N_frequencies)\n",
    "    error ~ InverseGamma(2,3)\n",
    "    \n",
    "    for i in eachindex(y)\n",
    "        t = x[i] - x[1]\n",
    "        s = seasonality_component(t, N_frequencies, beta)\n",
    "        y[i] ~ Normal(s, error+1e-5)\n",
    "    end\n",
    "\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try out different number of frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ps = []\n",
    "for N_frequencies in 1:4\n",
    "    res = optimize(seasonality_model(x, y_stationary, N_frequencies), MAP())\n",
    "    beta = res.values.array\n",
    "\n",
    "    mask = 1:60\n",
    "    p = plot(x[mask], y_stationary[mask], label=\"air passengers\", legend=:topleft, title=\"Number of frequencies: $N_frequencies\")\n",
    "    plot!(t -> seasonality_component(t, N_frequencies, beta), label=\"seasonality model MAP\")\n",
    "    push!(ps, p)\n",
    "end\n",
    "plot(ps...,layout=(2,2), size=(1200,800))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_frequencies = 3 # seems to be a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining the Trend and Seasonality Components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Simply combine trend and seasonality models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@model function prophet(x::Vector{Float64}, y::Vector{Float64}, N_frequencies::Int)\n",
    "    slope ~ Normal(0,3)\n",
    "    intercept ~ Normal(6,3)\n",
    "    error ~ InverseGamma(2,3)\n",
    "    n_changepoints = Int(ceil(length(y) / 12)) # changepoint at each year\n",
    "    tau ~ InverseGamma(1,1)\n",
    "    adjustments ~ filldist(Laplace(0,tau+1e-5), n_changepoints)\n",
    "    beta ~ filldist(Normal(0,1.), 2*N_frequencies)\n",
    "\n",
    "    k = slope\n",
    "    m = intercept\n",
    "    j = 1\n",
    "    for i in eachindex(y)\n",
    "        t = (x[i] - x[1])\n",
    "        if i % 12 == 1\n",
    "            k += adjustments[j]\n",
    "            m -= t * adjustments[j]\n",
    "            j += 1\n",
    "        end\n",
    "        s = seasonality_component(t, N_frequencies, beta)\n",
    "\n",
    "        y[i] ~ Normal(k * t + m + s, error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Solve for the MAP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_frequencies = 3\n",
    "x = air_passengers_2013_2018[!,\"Date\"]\n",
    "y = air_passengers_2013_2018[!,\"Total\"]\n",
    "changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "\n",
    "Turing.Random.seed!(1)\n",
    "map_estimate = optimize(prophet(x, y, N_frequencies), MAP(), Optim.Options(iterations=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function prophet_model_f(t, slope, intercept, changepoints, adjustments, N_frequencies, beta)\n",
    "    return trend_model_3_f(t, slope, intercept, changepoints, adjustments) + seasonality_component(t, N_frequencies, beta)\n",
    "end\n",
    "\n",
    "function plot_prophet_model(data, slope, intercept, error, adjustments, N_frequencies, beta)\n",
    "    p = base_plot(data)\n",
    "    x = data[!,\"Date\"]\n",
    "    t_min = x[1]\n",
    "    changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "    plot!(p, t -> prophet_model_f(t - t_min, slope, intercept, changepoints, adjustments, N_frequencies, beta), color=\"red\")\n",
    "    # plot!(p, t -> prophet_model_f(t - t_min, slope, intercept, changepoints, adjustments, N_frequencies, beta) + sqrt(error), color=\"orange\")\n",
    "    # plot!(p, t -> prophet_model_f(t - t_min, slope, intercept, changepoints, adjustments, N_frequencies, beta) - sqrt(error), color=\"orange\")\n",
    "\n",
    "    vline!(changepoints[abs.(adjustments) .> 0.01] .+ t_min, linestyle=:dash, color=\"black\")\n",
    "    p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_vector = map_estimate.values.array\n",
    "n_changepoints = length(y) ÷ 12\n",
    "\n",
    "slope, intercept, error, tau,  adjustments = map_vector[1:4]..., map_vector[5:(5+n_changepoints-1)]\n",
    "beta = map_vector[5+n_changepoints : end]\n",
    "\n",
    "plot_prophet_model(air_passengers_2013_2018, slope, intercept, error, adjustments, N_frequencies, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can be quite happy with the fit of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forecasting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we are also interested in forecasting the time series.\n",
    "\n",
    "To get accurate uncertainty quantification, we go back to our full Bayesian approach:   \n",
    "We approximate the full posterior instead of only optimising for the MAP.\n",
    "\n",
    "However, it is a good idea to set the MAP as starting point for the inference process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Turing.Random.seed!(0)\n",
    "res_prophet = sample(prophet(x, y, N_frequencies), NUTS(), 1000, progress=PRINT_PROGRESS, init_params=map_estimate.values.array);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we extrapolate into the future by continuing the trend, respecting the seasonality and randomly sampling future changepoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function prophet_forecast(x::Vector{Float64}, y::Vector{Float64},\n",
    "    m::Float64, k::Float64, error::Float64, N_frequencies::Int, beta::Vector{Float64},\n",
    "    forecast::Int)\n",
    "\n",
    "    tau = mean(abs, adjustments)\n",
    "\n",
    "    n_future_changepoints = ((length(y) + forecast) ÷ 12) - (length(y) ÷ 12)\n",
    "    future_adjustments = rand(filldist(Laplace(0,tau), n_future_changepoints))\n",
    "\n",
    "    y_pred = zeros(forecast)\n",
    "    Δ = x[2] - x[1]\n",
    "    x_future = x[end]\n",
    "    i = length(y)+1\n",
    "    j = 1\n",
    "    while i <= length(y) + forecast\n",
    "        x_future += Δ\n",
    "        t = (x_future - x[1])\n",
    "\n",
    "        if i % 12 == 1\n",
    "            k += future_adjustments[j]\n",
    "            m -= t * future_adjustments[j]\n",
    "            j += 1\n",
    "        end\n",
    "\n",
    "        s = seasonality_component(t, N_frequencies, beta)\n",
    "        y_pred[i-length(y)] = rand(Normal(k * t + m + s, error))\n",
    "        i += 1\n",
    "    end\n",
    "\n",
    "    return y_pred\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We write a helper function to extrapolate randomly `n_samples_per_trace` times for each sample in the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function sample_forecast(res, changepoints, forecast, n_samples_per_trace)\n",
    "    n_changepoints = length(changepoints)\n",
    "    y_pred = zeros(n_samples_per_trace * length(res), forecast)\n",
    "\n",
    "    j = 0\n",
    "    for i in 1:length(res)\n",
    "        trace = res[i].value.data\n",
    "\n",
    "        slope, intercept, error, tau,  adjustments = trace[1:4]..., trace[5:(5+n_changepoints-1)]\n",
    "        beta = trace[5+n_changepoints : end]\n",
    "\n",
    "        for _ in 1:n_samples_per_trace\n",
    "            j += 1\n",
    "            k = slope + sum(adjustments)\n",
    "            m = intercept - adjustments'changepoints\n",
    "            y_pred[j,:] = prophet_forecast(x,y,m,k,error,N_frequencies,beta,forecast)\n",
    "        end\n",
    "    end\n",
    "    return y_pred\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, with plotting the forecast, we can check if we are satisfied with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function plot_forecast(x, y, x_forecast, y_pred)\n",
    "    forecast = length(x_forecast)\n",
    "    p = plot(x, y, label=\"air passenger historic data\", xlabel=\"year\", ylabel=\"passenger [10^7]\", legend=:topleft)\n",
    "    q05 = map(i -> quantile(y_pred[:,i], 0.05), 1:forecast)\n",
    "    q50 = map(i -> quantile(y_pred[:,i], 0.5), 1:forecast)\n",
    "    q95 = map(i -> quantile(y_pred[:,i], 0.95), 1:forecast)\n",
    "    plot!(x_forecast, q50, ribbon=(q50-q05,q95-q50), label=\"forecast\")\n",
    "    return p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_year_forecast = 4\n",
    "forecast = n_year_forecast * 12\n",
    "x_forecast = collect(maximum(x) .+ (1:forecast)./12)\n",
    "\n",
    "y_pred = sample_forecast(res_prophet, changepoints, forecast, 10)\n",
    "plot_forecast(x, y, x_forecast, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: How to model the pandemic ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_plot(air_passengers_2013_2023)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we apply the unmodified model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_frequencies = 3\n",
    "x = air_passengers_2013_2023[!,\"Date\"]\n",
    "y = air_passengers_2013_2023[!,\"Total\"]\n",
    "changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "\n",
    "Turing.Random.seed!(0)\n",
    "map_estimate = optimize(prophet(x, y, N_frequencies), MAP(), Optim.Options(iterations=10000))\n",
    "\n",
    "Turing.Random.seed!(0)\n",
    "res_prophet_2023 = sample(prophet(x, y, N_frequencies), NUTS(), 1000, progress=PRINT_PROGRESS, init_params=map_estimate.values.array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_vector = map_estimate.values.array\n",
    "n_changepoints = length(y) ÷ 12\n",
    "\n",
    "slope, intercept, error, tau,  adjustments = map_vector[1:4]..., map_vector[5:(5+n_changepoints-1)]\n",
    "beta = map_vector[5+n_changepoints : end]\n",
    "\n",
    "plot_prophet_model(air_passengers_2013_2023, slope, intercept, error, adjustments, N_frequencies, beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thanks to our fully Bayesian approach, the uncertainty around our prediction is know very large,  \n",
    "reflecting the fact that the model does not fit the data well anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_year_forecast = 4\n",
    "forecast = n_year_forecast * 12\n",
    "x_forecast = collect(maximum(x) .+ (1:forecast)./12)\n",
    "\n",
    "y_pred = sample_forecast(res_prophet_2023, changepoints, forecast, 10)\n",
    "plot_forecast(x, y, x_forecast, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We upgrade the model to specifically treat the years of the pandemic as one-off abnormalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@model function prophet_covid(x::Vector{Float64}, y::Vector{Float64}, N_frequencies::Int)\n",
    "    slope ~ Normal(0,3)\n",
    "    intercept ~ Normal(6,3)\n",
    "    error ~ InverseGamma(2,3)\n",
    "    n_changepoints = Int(ceil(length(y) / 12)) # changepoint at each year\n",
    "    tau ~ InverseGamma(1,1)\n",
    "    adjustments ~ filldist(Laplace(0,tau+1e-5), n_changepoints)\n",
    "    beta ~ filldist(Normal(0,1.), 2*N_frequencies)\n",
    "\n",
    "    # UPGRADE: latent shock variables for the years 2021 to 2023\n",
    "    shock_2020 ~ Normal(-8,1)\n",
    "    shock_2021 ~ Normal(-4,1)\n",
    "    shock_2022 ~ Normal(-2,1)\n",
    "\n",
    "    k = slope\n",
    "    m = intercept\n",
    "    j = 1\n",
    "    for i in eachindex(y)\n",
    "        t = (x[i] - x[1])\n",
    "        if i % 12 == 1\n",
    "            k += adjustments[j]\n",
    "            m -= t * adjustments[j]\n",
    "            j += 1\n",
    "        end\n",
    "        s = seasonality_component(t, N_frequencies, beta)\n",
    "\n",
    "\n",
    "        # UPGRADE: extra \"intercept\" variables for the years 2021 to 2023\n",
    "        if 2020 ≤ x[i] && x[i] < 2021\n",
    "            d = shock_2020\n",
    "        elseif 2021 ≤ x[i] && x[i] < 2022\n",
    "            d = shock_2021\n",
    "        elseif 2022 ≤ x[i] && x[i] < 2023\n",
    "            d = shock_2022\n",
    "        else\n",
    "            d = 0\n",
    "        end\n",
    "\n",
    "        y[i] ~ Normal(k * t + m + s + d, error+1e-5)\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model again. First MAP, then full posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_frequencies = 3\n",
    "x = air_passengers_2013_2023[!,\"Date\"]\n",
    "y = air_passengers_2013_2023[!,\"Total\"]\n",
    "changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "\n",
    "Turing.Random.seed!(0)\n",
    "map_estimate = optimize(prophet_covid(x, y, N_frequencies), MAP(), Optim.Options(iterations=10000))\n",
    "\n",
    "Turing.Random.seed!(0)\n",
    "res_prophet_covid = sample(prophet_covid(x, y, N_frequencies), NUTS(), 1000, progress=PRINT_PROGRESS, init_params=map_estimate.values.array);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "function shock_component(t::Float64, shocks::Vector{Float64})\n",
    "    if 2020 ≤ t && t < 2021\n",
    "        d = shocks[1]\n",
    "    elseif 2021 ≤ t && t < 2022\n",
    "        d = shocks[2]\n",
    "    elseif 2022 ≤ t && t < 2023\n",
    "        d = shocks[3]\n",
    "    else\n",
    "        d = 0\n",
    "    end\n",
    "end\n",
    "\n",
    "function plot_prophet_covid_model(data, slope, intercept, error, adjustments, N_frequencies, beta, shocks)\n",
    "    p = base_plot(data)\n",
    "    x = data[!,\"Date\"]\n",
    "    t_min = x[1]\n",
    "    changepoints = x[(1:length(x)) .% 12 .== 1] .- x[1]\n",
    "    plot!(p, t -> prophet_model_f(t - t_min, slope, intercept, changepoints, adjustments, N_frequencies, beta) + shock_component(t,shocks), color=\"red\")\n",
    "\n",
    "    vline!(changepoints[abs.(adjustments) .> 0.01] .+ t_min, linestyle=:dash, color=\"black\")\n",
    "    p\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "map_vector = map_estimate.values.array\n",
    "n_changepoints = length(y) ÷ 12\n",
    "\n",
    "slope, intercept, error, tau, adjustments = map_vector[1:4]..., map_vector[5:(5+n_changepoints-1)]\n",
    "beta = map_vector[5+n_changepoints : 5+n_changepoints + 2*N_frequencies-1]\n",
    "shocks = map_vector[5+n_changepoints + 2*N_frequencies : end]\n",
    "\n",
    "plot_prophet_covid_model(air_passengers_2013_2023, slope, intercept, error, adjustments, N_frequencies, beta, shocks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, after explicetly modelling the pandemic, our uncertainty estimates get tighter again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_year_forecast = 4\n",
    "forecast = n_year_forecast * 12\n",
    "x_forecast = collect(maximum(x) .+ (1:forecast)./12)\n",
    "\n",
    "y_pred = sample_forecast(res_prophet_covid, changepoints, forecast, 10)\n",
    "plot_forecast(x, y, x_forecast, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.9.1 (4 threads)",
   "language": "julia",
   "name": "julia-1.9_4-threads"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
